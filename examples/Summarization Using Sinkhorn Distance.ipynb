{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_distances, euclidean_distances\n",
    "\n",
    "import spacy\n",
    "import ot\n",
    " \n",
    "import seaborn as sns\n",
    "\n",
    "from mlutil.embeddings import load_gensim_embedding_model, TextEncoderVectorizer, WordEmbeddingsVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘wikihowAll.csv’ already there; not retrieving.\n"
     ]
    }
   ],
   "source": [
    "!wget -nc -O wikihowAll.csv https://query.data.world/s/lult233wfonljfadtexn2t5x5rb7is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_analyzer(sentence, stop_words=[], excluded_pos=['PUNCT', 'SPACE', 'PRON', 'X'], lowercase=True, lemmatize=True):\n",
    "    def preprocess_token(token):\n",
    "        if lemmatize:\n",
    "            word = token.lemma_\n",
    "        else:\n",
    "            word = token.text\n",
    "        if lowercase:\n",
    "            return word.lower()\n",
    "        else:\n",
    "            return word\n",
    "    \n",
    "    if lowercase:\n",
    "        words = [preprocess_token(token) for token in sentence if token.pos_ not in excluded_pos]\n",
    "    else:\n",
    "        words = [preprocess_token(token) for token in sentence if token.pos_ not in excluded_pos]\n",
    "    return [token for token in words if not token in stop_words]\n",
    "\n",
    "\n",
    "def make_uniform_like(n):\n",
    "    return np.ones(n) / n\n",
    "\n",
    "\n",
    "def get_masked_p_doc(i, p_doc):\n",
    "    ith_doc_indices_start = sum(mat.shape[0] for mat in sentence_matrices[:i])\n",
    "    ith_doc_indices_end = ith_doc_indices_start + sentence_matrices[i].shape[0]\n",
    "    \n",
    "    mask = np.ones_like(p_doc)\n",
    "    mask[ith_doc_indices_start:ith_doc_indices_end] = 0\n",
    "    p_doc = p_doc * mask\n",
    "    return p_doc / p_doc.sum()\n",
    "\n",
    "\n",
    "def transport_matrix(i, mask_p_doc=True, reg=1):\n",
    "    n, m = transport_costs[i].shape\n",
    "    p_example = make_uniform_like(n)\n",
    "    p_doc = make_uniform_like(m)\n",
    "    if mask_p_doc:\n",
    "        p_doc = get_masked_p_doc(i, p_doc)\n",
    "    return ot.sinkhorn(p_example, p_doc, transport_costs[i], reg)\n",
    "\n",
    "\n",
    "def transport_cost(i, reg=1):\n",
    "    return (transport_costs[i] * transport_matrix(i, reg)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikihow_df = pd.read_csv('wikihowAll.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/etc/miniconda/envs/ml/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "keyed_vectors = load_gensim_embedding_model('glove-wiki-gigaword-50')\n",
    "vectorizer = WordEmbeddingsVectorizer(keyed_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/etc/miniconda/envs/ml/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "small_wikihow_df = wikihow_df[:100]\n",
    "small_wikihow_df['text'] = small_wikihow_df['text'].str.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = small_wikihow_df.iloc[0]['text'].replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence = list(doc.sents)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sent = list(doc.sents)[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sentence_length = 6\n",
    "\n",
    "sentences = list(doc.sents)\n",
    "analyzed_sentences = [sent_analyzer(sent, stop_words=nlp.Defaults.stop_words) for sent in sentences]\n",
    "cleaned_sentences = [' '.join(sent) for sent in analyzed_sentences if len(sent) >= min_sentence_length]\n",
    "filtered_sentences = [sent for (sent, analyzed_sent) in zip(sentences, analyzed_sentences) if len(analyzed_sent) >= min_sentence_length]\n",
    "sentence_matrices = vectorizer.transform(cleaned_sentences, aggregate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_document = np.vstack(sentence_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "transport_costs = [cosine_distances(sent, whole_document) for sent in sentence_matrices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculated_costs = [transport_cost(i) for i in range(len(sentence_matrices))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7489980524374757"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculated_costs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13, 25,  3, 23, 12, 21, 22, 24, 16, 20, 15,  7,  5,  8, 14, 19,  1,\n",
       "        2, 10, 18,  6,  9,  4, 17, 11,  0])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_scores = np.argsort(calculated_costs)\n",
    "sorted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_matrices[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 th most central sentence\n",
      "Cheap and easy, this is also a good way to handle papers and ideas you touch regularly or need to pin up and down for inspiration.\n",
      "cheap easy good way handle paper idea touch regularly need pin inspiration\n",
      "\n",
      "2 th most central sentence\n",
      "If you haven't used it in the last six months there is little chance you'll use it in the next six months.\n",
      "use month little chance use month\n",
      "\n",
      "3 th most central sentence\n",
      "Some ideas include:   Essential supplies area -- the things you use every day.\n",
      "idea include essential supply area thing use day\n",
      "\n",
      "4 th most central sentence\n",
      "This is a good thing, but only if you set aside time to declutter.\n",
      "good thing set aside time declutter\n",
      "\n",
      "5 th most central sentence\n",
      "Simply string up the wires across a wall or along the ceiling and use them to hold essential papers that you don't want to cut or ruin with tacks or tape.\n",
      "simply stre wire wall ceiling use hold essential paper want cut ruin tack tape\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(i+1, 'th most central sentence')\n",
    "    print(filtered_sentences[sorted_scores[i]])\n",
    "    print(cleaned_sentences[sorted_scores[i]])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cheap easy good way handle paper idea -pron- touch regularly need pin inspiration'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_sentences[sorted_scores[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Some ideas, beyond those just mentioned, include:   Canvas shoe racks on the back of the door Wine racks with cups in each slot to hold pens/pencils."
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[sorted_scores[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sentences = whole_document.shape[0]\n",
    "p_sentences = make_uniform_like(n_sentences)\n",
    "K = cosine_distances(whole_document, whole_document)\n",
    "transport_matrix = ot.sinkhorn(p_sentences, p_sentences, K, 0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
